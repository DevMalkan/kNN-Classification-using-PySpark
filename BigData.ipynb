{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyODjmHRQJbjxCdcJN700eS0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xze6enbrM0dl","executionInfo":{"status":"ok","timestamp":1700719834022,"user_tz":-330,"elapsed":27996,"user":{"displayName":"DEV HITESH MALKAN","userId":"11485949328858706123"}},"outputId":"9ad4479a-68cd-40d0-97eb-8e42dd96a91b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"phld7OLHNrqj","executionInfo":{"status":"ok","timestamp":1700719894899,"user_tz":-330,"elapsed":57967,"user":{"displayName":"DEV HITESH MALKAN","userId":"11485949328858706123"}},"outputId":"3754f8fb-297c-4cef-dc09-734d710b23df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=8d4c8e9e091c4f0c2b45c824046aaf9b040588f47226518f3315105d0992f152\n","  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.0\n"]}]},{"cell_type":"code","execution_count":55,"metadata":{"id":"fQt6tFcso4VG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700735992122,"user_tz":-330,"elapsed":15529,"user":{"displayName":"DEV HITESH MALKAN","userId":"11485949328858706123"}},"outputId":"e706ffcb-b5a2-469c-bb4a-737e1086e2ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted   0   1  2  3  4  8  9  All\n","Actual                               \n","0          30  10  1  0  0  0  0   41\n","1          13  20  2  0  0  0  0   35\n","2           2   3  3  0  0  0  0    8\n","3           1   1  0  2  0  0  0    4\n","4           0   0  0  0  1  0  0    1\n","6           1   0  0  0  0  0  0    1\n","8           0   0  0  0  0  5  0    5\n","9           0   0  0  0  0  0  5    5\n","All        47  34  6  2  1  5  5  100\n"]}],"source":["from pyspark import SparkContext, SparkConf\n","from pyspark.sql import SparkSession\n","import heapq\n","from heapq import heapify, heappush, heappop\n","import sys\n","import math\n","import numpy as np\n","import pandas as pd\n","\n","def calculate_min_max(data):\n","    # Calculate min and max values for each feature\n","    feature_min_max = [(float('inf'), float('-inf')) for _ in range(len(data[0].split(',')))]\n","    for line in data:\n","        features = [float(x) for x in line.split(',')]\n","        for i, value in enumerate(features):\n","            feature_min_max[i] = (min(feature_min_max[i][0], value), max(feature_min_max[i][1], value))\n","    return feature_min_max\n","\n","def normalize(line, feature_min_max):\n","    # Normalize each feature in the line using (X - Xmin) / (Xmax - Xmin)\n","    features = [float(x) for x in line.split(',')]\n","    normalized_features = [(features[i] - feature_min_max[i][0]) / (feature_min_max[i][1] - feature_min_max[i][0])\n","                           for i in range(len(features)-1)]\n","    normalized_features.append(int(features[10]))\n","    return normalized_features\n","\n","def distance(x,y):\n","    sum = 0.0\n","    for i in range(10):\n","      sum += (x[i]-y[i])*(x[i]-y[i])\n","    return math.sqrt(sum)\n","\n","def kNN(training_set, test_set, k):\n","\n","    result = []\n","\n","    for i in range(len(test_set)):\n","      d = []\n","      c = []\n","      x = test_set[i][1]\n","      max_heap = []\n","      heapq.heapify(max_heap)\n","      for j in range(len(training_set)):\n","        y = training_set[j]\n","        dist = distance(x,y)\n","        heappush(max_heap,(-dist,(dist,y[10])))\n","        #heappush(max_heap,(dist,y[10]))\n","        if (len(max_heap) > k):\n","          heapq.heappop(max_heap)\n","      while max_heap:\n","        out = heapq.heappop(max_heap)\n","        d.append(-out[0])\n","        c.append(out[1][1])\n","      result.append((test_set[i][0],(d,c)))\n","    return result\n","\n","def calIter(tr_weight, ts_weight, mem_allow):\n","    numIterations = 0;\n","    weightTrain = (8 * tr_weight * numFeatures) / (num_mappers * 1024.0 * 1024.0)\n","    weightTest = (8 * ts_weight * numFeatures) / (1024.0 * 1024.0)\n","    if (weightTrain + weightTest < mem_allow * 1024.0):\n","          numIterations = 1\n","    else:\n","      if (weightTrain >= mem_allow * 1024.0):\n","        print(\"Train weight bigger than lim-task. Abort\")\n","        sys.exit(1)\n","\n","      numIterations = int((1 + (weightTest / ((mem_allow * 1024.0) - weightTrain))))\n","    return numIterations\n","\n","def combineResult(result1, result2):\n","\n","    d1 = result1[0]\n","    d2 = result2[0]\n","    c1 = result1[1]\n","    c2 = result2[1]\n","    d = []\n","    c = []\n","    j1 = len(d1)-1\n","    j2 = len(d2)-1\n","    for i in range(len(d1)):\n","      if (j2 >= 0 and j1 >= 0 and d1[j1] < d2[j2]) or j2 < 0 :\n","        d.append(d1[j1])\n","        c.append(c1[j1])\n","        j1 = j1-1\n","      elif (j1 >= 0 and j2 >= 0 and d2[j2] <= d1[j1]) or j1 < 0:\n","        d.append(d2[j2])\n","        c.append(c2[j2])\n","        j2 = j2-1\n","    return (d,c)\n","\n","def calculateRightPredicted(result, test_set):\n","\n","    output = []\n","    first = test_set[0][0]\n","    for i in range(len(result)):\n","      c = result[i][1][1]\n","      index = result[i][0]-first\n","      count = np.zeros(10)\n","      maxVotes = 0\n","      predicted = -1\n","      for j in range(k):\n","        count[c[j]] = count[c[j]]+1\n","        if count[c[j]] > maxVotes:\n","          maxVotes = count[c[j]]\n","          predicted = c[j]\n","      actual = test_set[index][1][10]\n","      output.append((actual,predicted))\n","    return output\n","\n","\n","def calculateConfusionMatrix(right_predicted_classes):\n","    actual_list = []\n","    predicted_list = []\n","\n","    for i in range(len(right_predicted_classes)):\n","        for j in range(len(right_predicted_classes[i])):\n","            actual_list.append(right_predicted_classes[i][j][0])\n","            predicted_list.append(right_predicted_classes[i][j][1])\n","\n","    # Create Series from lists\n","    y_actu = pd.Series(actual_list, name='Actual', dtype=int)\n","    y_pred = pd.Series(predicted_list, name='Predicted', dtype=int)\n","    df_confusion = pd.crosstab(y_actu, y_pred, margins=True)\n","    return df_confusion\n","\n","\n","if __name__ == \"__main__\":\n","    conf = SparkConf().setAppName(\"kNN-IS\")\n","    sc = SparkContext(conf=conf)\n","    spark = SparkSession(sc)\n","\n","    # Load training dataset with a specified number of partitions (mappers)\n","    num_mappers = 2  # Set the desired number of mappers\n","    numFeatures = 10\n","    k = 1\n","    TR_RDD_raw = sc.textFile(\"/content/drive/MyDrive/datasets/poker-hand-training-true.data\", num_mappers)\n","    feature_min_max = calculate_min_max(TR_RDD_raw.collect())  # Collect data to calculate min and max\n","    TR_RDD = TR_RDD_raw.map(lambda line: normalize(line, feature_min_max)).cache()\n","    #TR_RDD.collect()\n","    #print(TR_RDD.take(100))\n","\n","    # Load and zip test dataset with index\n","    TS_RDD_raw = sc.textFile(\"/content/drive/MyDrive/datasets/test1.data\")\n","    TS_RDD_raw_with_index = TS_RDD_raw.zipWithIndex()\n","    TS_RDD = TS_RDD_raw_with_index.map(lambda x: (x[1], normalize(x[0], feature_min_max))).cache()\n","\n","    #TS_RDD.collect()\n","    #print(TS_RDD.take(100))\n","\n","    # Get weights for iterations\n","    tr_weight = TR_RDD.count()\n","    ts_weight = TS_RDD.count()\n","    mem_allow = 0.2\n","    iterations = calIter(tr_weight, ts_weight, mem_allow)\n","\n","    # Range partitioning for TS_RDD\n","    TS_RDD.partitionBy(iterations,partitionFunc=range)\n","    all_partitions = TS_RDD.glom().collect()\n","    #print(len(all_partitions[0]))\n","\n","    right_predicted_classes = []\n","\n","    for i in range(len(all_partitions)):\n","        # Broadcast TS_i\n","        TS_i = spark.sparkContext.broadcast(all_partitions[i])\n","\n","        # MapPartition to perform kNN\n","        resultKNN = TR_RDD.mapPartitions(lambda tr_partition: kNN(list(tr_partition), TS_i.value, k))\n","        #resultKNN.collect()\n","        #print(resultKNN.take(10))\n","\n","        # ReduceByKey to combine results\n","        result = resultKNN.reduceByKey(lambda result1,result2: combineResult(result1,result2)).collect()\n","        #print(result)\n","\n","        # Calculate right predicted classes for this iteration\n","        right_predicted_classes.append(calculateRightPredicted(result,TS_i.value))\n","\n","    #print(right_predicted_classes)\n","\n","    # Calculate confusion matrix\n","    confusion_matrix = calculateConfusionMatrix(right_predicted_classes)\n","    print(confusion_matrix)\n","\n","    # Stop the Spark context\n","    sc.stop()\n"]},{"cell_type":"code","source":["sc.stop()"],"metadata":{"id":"CvzMhCQEPuNp","executionInfo":{"status":"ok","timestamp":1700730720744,"user_tz":-330,"elapsed":798,"user":{"displayName":"DEV HITESH MALKAN","userId":"11485949328858706123"}}},"execution_count":42,"outputs":[]}]}